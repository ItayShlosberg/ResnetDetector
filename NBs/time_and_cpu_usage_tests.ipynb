{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "superior-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "sys.path.append(r'C:\\Users\\itay\\Desktop\\IDF')\n",
    "\n",
    "from utils.bboxes_conversions import *\n",
    "from utils.metrics import *\n",
    "from utils.postprocesses import *\n",
    "from utils.preprocesses import *\n",
    "# from pympler import asizeof\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "endless-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_n = YOLO(\"yolov8n.pt\")\n",
    "yolo_s = YOLO(\"yolov8s.pt\")\n",
    "yolo_m = YOLO(\"yolov8m.pt\")\n",
    "yolo_l = YOLO(\"yolov8l.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "practical-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo_n.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "romantic-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round(asizeof.asizeof(yolo_nas_n)/1024**2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "official-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = fr'C:\\Users\\itay\\Desktop\\IDF\\datasets\\COCO\\val2017'\n",
    "IMAGE_NAME = fr'000000015335.jpg'\n",
    "image_path = os.path.join(IMAGES_PATH, IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "activated-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.open(image_path)\n",
    "image = Image.open(os.path.join(IMAGES_PATH, IMAGE_NAME))\n",
    "img_array = np.array(image)\n",
    "img_tensor = torch.from_numpy(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "nervous-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 4 persons, 1 cup, 113.7ms\n",
      "Speed: 2.0ms preprocess, 113.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 cup, 1 bowl, 149.6ms\n",
      "Speed: 2.0ms preprocess, 149.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 1 cup, 1 dining table, 310.2ms\n",
      "Speed: 1.0ms preprocess, 310.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 8 persons, 1 cup, 2 couchs, 1 cell phone, 532.6ms\n",
      "Speed: 1.0ms preprocess, 532.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO_V8_n Inference time: 0.12 seconds, 8.22FPS\n",
      "YOLO_V8_s Inference time: 0.16 seconds, 6.43FPS\n",
      "YOLO_V8_m Inference time: 0.32 seconds, 3.14FPS\n",
      "YOLO_V8_l Inference time: 0.54 seconds, 1.85FPS\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  \n",
    "    start_time = time.time()\n",
    "    res = yolo_n(image)\n",
    "    end_time = time.time()\n",
    "    n_inference_time = end_time - start_time\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    res = yolo_s(image)\n",
    "    end_time = time.time()\n",
    "    s_inference_time = end_time - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    res = yolo_m(image)\n",
    "    end_time = time.time()\n",
    "    m_inference_time = end_time - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    res = yolo_l(image)\n",
    "    end_time = time.time()\n",
    "    l_inference_time = end_time - start_time\n",
    "\n",
    "print(f\"YOLO_V8_n Inference time: {n_inference_time:.2f} seconds, {1/n_inference_time:.2f}FPS\")    \n",
    "print(f\"YOLO_V8_s Inference time: {s_inference_time:.2f} seconds, {1/s_inference_time:.2f}FPS\")\n",
    "print(f\"YOLO_V8_m Inference time: {m_inference_time:.2f} seconds, {1/m_inference_time:.2f}FPS\")\n",
    "print(f\"YOLO_V8_l Inference time: {l_inference_time:.2f} seconds, {1/l_inference_time:.2f}FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "absolute-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "169\n",
      "219\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "print(len(list(yolo_n.modules())))\n",
    "print(len(list(yolo_s.modules())))\n",
    "print(len(list(yolo_m.modules())))\n",
    "print(len(list(yolo_l.modules())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "intensive-coordination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "115\n",
      "155\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "print(len([k for k,v in yolo_n.state_dict().items() if 'conv' in k]))\n",
    "print(len([k for k,v in yolo_s.state_dict().items() if 'conv' in k]))\n",
    "print(len([k for k,v in yolo_m.state_dict().items() if 'conv' in k]))\n",
    "print(len([k for k,v in yolo_l.state_dict().items() if 'conv' in k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "other-montana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18*36\n",
    "\n",
    "18*18\n",
    "\n",
    "144*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "unauthorized-newport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo_n: 1.84MB\n",
      "yolo_s: 1.84MB\n",
      "yolo_m: 2.1MB\n",
      "yolo_l: 2.35MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'yolo_n: {round(asizeof.asizeof(yolo_n)/1024**2, 2)}MB')\n",
    "print(f'yolo_s: {round(asizeof.asizeof(yolo_s)/1024**2, 2)}MB')\n",
    "print(f'yolo_m: {round(asizeof.asizeof(yolo_m)/1024**2, 2)}MB')\n",
    "print(f'yolo_l: {round(asizeof.asizeof(yolo_l)/1024**2, 2)}MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "productive-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = yolo_n.state_dict().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "nutritional-timber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.1884765625"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3151904 * 32/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "gorgeous-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model_params)[0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "seasonal-prairie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.19MB\n",
      "340.47MB\n",
      "789.98MB\n",
      "1332.65MB\n"
     ]
    }
   ],
   "source": [
    "print(f'{sum([np.product(v.shape) for k,v in dict(yolo_n.state_dict().items()).items()])* 32/1024**2:.2f}MB')\n",
    "print(f'{sum([np.product(v.shape) for k,v in dict(yolo_s.state_dict().items()).items()])* 32/1024**2:.2f}MB')\n",
    "print(f'{sum([np.product(v.shape) for k,v in dict(yolo_m.state_dict().items()).items()])* 32/1024**2:.2f}MB')\n",
    "print(f'{sum([np.product(v.shape) for k,v in dict(yolo_l.state_dict().items()).items()])* 32/1024**2:.2f}MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
